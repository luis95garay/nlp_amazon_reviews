{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luisg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luisg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\luisg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\luisg\\Documents\\projects\\nlp_amazon_reviews\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "import bz2\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt') # At first you have to download these nltk packages.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "# Mlflow\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.ft.txt.bz2', 'train.ft.txt.bz2']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_texts(file):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "train_labels, train_texts = get_labels_and_texts('../data/train.ft.txt.bz2')\n",
    "test_labels, test_texts = get_labels_and_texts('../data/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_texts = train_labels[:10000], train_texts[:10000]\n",
    "test_labels, test_texts = test_labels[:2000], test_texts[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') # defining stop_words\n",
    "stop_words.remove('not') # removing not from the stop_words list as it contains value in negative movies\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(review):\n",
    "\n",
    "  # data cleaning\n",
    "  review = re.sub(re.compile('<.*?>'), '', review) #removing html tags\n",
    "  review =  re.sub('[^A-Za-z0-9]+', ' ', review) #taking only words\n",
    "  review = re.sub(r\"http\\S+\", \"\",review) #Removing URLs \n",
    "  \n",
    "  # lowercase\n",
    "  review = review.lower()\n",
    "  \n",
    "  # tokenization\n",
    "  tokens = nltk.word_tokenize(review) # converts review to tokens\n",
    "  \n",
    "  # stop_words removal\n",
    "  review = [word for word in tokens if word not in stop_words] #removing stop words\n",
    "  \n",
    "  # lemmatization\n",
    "  # review = [lemmatizer.lemmatize(word) for word in review]\n",
    "  \n",
    "  # join words in preprocessed review\n",
    "  review = ' '.join(review)\n",
    "  \n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          u\"\\U00002702-\\U000027B0\"\n",
    "                          u\"\\U000024C2-\\U0001F251\"\n",
    "                          \"]+\", flags=re.UNICODE)\n",
    "  review = emoji_pattern.sub(r'', review) #Removing emojis\n",
    "  \n",
    "  return review\n",
    "\n",
    "\n",
    "def get_max_sentence_length(sentences_list: List, tokenizer):\n",
    "    max_len = 0\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences_list:\n",
    "\n",
    "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "        # Update the maximum sentence length.\n",
    "        max_len = max(max_len, len(input_ids))\n",
    "\n",
    "    print('Max sentence length: ', max_len)\n",
    "    \n",
    "    return max_len\n",
    "\n",
    "\n",
    "def bert_preprocessing(sentences_list: List, tokenizer, max_length, labels):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences_list:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_length,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    # print('Original: ', sentences_list[0])\n",
    "    # print('Token IDs:', input_ids[0])\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "def get_data_loaders(input_ids, attention_masks, labels, input_ids_test, attention_masks_test, labels_test):\n",
    "    val_ratio = 0.2\n",
    "    # Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "    batch_size = 32\n",
    "\n",
    "    # Indices of the train and validation splits stratified by labels\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        np.arange(len(labels)),\n",
    "        test_size = val_ratio,\n",
    "        shuffle = True,\n",
    "        stratify = labels)\n",
    "\n",
    "    # Train and validation sets\n",
    "    train_set = TensorDataset(input_ids[train_idx], \n",
    "                            attention_masks[train_idx], \n",
    "                            labels[train_idx])\n",
    "\n",
    "    val_set = TensorDataset(input_ids[val_idx], \n",
    "                            attention_masks[val_idx], \n",
    "                            labels[val_idx])\n",
    "\n",
    "    test_set = TensorDataset(input_ids_test, \n",
    "                            attention_masks_test, \n",
    "                            labels_test)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "                train_set,\n",
    "                sampler = RandomSampler(train_set),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_set,\n",
    "                sampler = SequentialSampler(val_set),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "                test_set,\n",
    "                sampler = SequentialSampler(test_set),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "\n",
    "    print('{:>5,} training samples'.format(len(train_idx)))\n",
    "    print('{:>5,} validation samples'.format(len(val_idx)))\n",
    "    print('{:>5,} test samples'.format(len(input_ids_test)))\n",
    "\n",
    "    return train_dataloader, validation_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisg\\Documents\\projects\\nlp_amazon_reviews\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prep_train_texts = [data_preprocessing(text) for text in train_texts]\n",
    "max_sentence_length = get_max_sentence_length(prep_train_texts, tokenizer)\n",
    "input_ids, attention_masks, labels = bert_preprocessing(prep_train_texts, tokenizer, max_sentence_length, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_test_texts = [data_preprocessing(text) for text in test_texts]\n",
    "input_ids_test, attention_masks_test, labels_test = bert_preprocessing(prep_test_texts, tokenizer, max_sentence_length, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,000 training samples\n",
      "2,000 validation samples\n",
      "2,000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, validation_dataloader, test_dataloader = get_data_loaders(input_ids, attention_masks, labels, input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(' Original: ', prep_test_texts[0])\n",
    "\n",
    "# # Print the sentence split into tokens.\n",
    "# print('Tokenized: ', tokenizer.tokenize(prep_test_texts[0]))\n",
    "\n",
    "# # Print the sentence mapped to token ids.\n",
    "# print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prep_test_texts[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    \n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(train_dataloader, validation_dataloader, test_dataloader, model, params):\n",
    "    \n",
    "    with mlflow.start_run(run_name=params['run_name']) as run:\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "        if train_on_gpu:\n",
    "            model.cuda()\n",
    "\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                        )\n",
    "\n",
    "        # Number of training epochs. The BERT authors recommend between 2 and 4.\n",
    "        epochs = params['n_epochs']\n",
    "\n",
    "        # Total number of training steps is [number of batches] x [number of epochs].\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "        # Create the learning rate scheduler.\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)\n",
    "\n",
    "        training_stats = []\n",
    "\n",
    "        # Measure the total training time for the whole run.\n",
    "        total_t0 = time.time()\n",
    "\n",
    "        valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "        # For each epoch...\n",
    "        for epoch_i in range(0, epochs):\n",
    "\n",
    "            # ========================================\n",
    "            #               Training\n",
    "            # ========================================\n",
    "\n",
    "            # Perform one full pass over the training set.\n",
    "\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            print('Training...')\n",
    "\n",
    "            # Measure how long the training epoch takes.\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Reset the total loss for this epoch.\n",
    "            total_train_loss = 0\n",
    "            total_eval_loss = 0\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for batch in train_dataloader:\n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "                b_input_ids = batch[0].type(torch.LongTensor).to(device)\n",
    "                b_input_mask = batch[1].type(torch.LongTensor).to(device)\n",
    "                b_labels = batch[2].type(torch.LongTensor).to(device)\n",
    "\n",
    "                model.zero_grad()\n",
    "\n",
    "                output = model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels\n",
    "                )\n",
    "\n",
    "                total_train_loss += output.loss.item()\n",
    "                output.loss.backward()\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            # Measure how long this epoch took.\n",
    "            training_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "            # ========================================\n",
    "            #               Validation\n",
    "            # ========================================\n",
    "            # After the completion of each training epoch, measure our performance on\n",
    "            # our validation set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "                b_input_ids = batch[0].type(torch.LongTensor).to(device)\n",
    "                b_input_mask = batch[1].type(torch.LongTensor).to(device)\n",
    "                b_labels = batch[2].type(torch.LongTensor).to(device)\n",
    "\n",
    "                # Tell pytorch not to bother with constructing the compute graph during\n",
    "                # the forward pass, since this is only needed for backprop (training).\n",
    "                with torch.no_grad():\n",
    "                    output = model(\n",
    "                        b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels\n",
    "                    )\n",
    "\n",
    "                # Accumulate the validation loss.\n",
    "                total_eval_loss += output.loss.item()\n",
    "\n",
    "            # Calculate the average loss over all of the batches.\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "\n",
    "            # print training/validation statistics \n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t\\Time: {:}'.format(\n",
    "                epoch_i, avg_train_loss, avg_val_loss, validation_time))\n",
    "\n",
    "            mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch_i + 1)\n",
    "            mlflow.log_metric(\"valid_loss\", avg_val_loss, step=epoch_i + 1)\n",
    "    \n",
    "            # save model if validation loss has decreased\n",
    "            if avg_val_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                avg_val_loss))\n",
    "                output_dir = '../models/bert'\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                model_to_save.save_pretrained(output_dir)\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                valid_loss_min = avg_val_loss\n",
    "\n",
    "\n",
    "        # test\n",
    "        # track test loss\n",
    "\n",
    "        # ========================================\n",
    "        #               Testing\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Testing...\")\n",
    "\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        class_correct = list(0. for i in range(len(params['class_names'])))\n",
    "        class_total = list(0. for i in range(len(params['class_names'])))\n",
    "        \n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for idx, batch in enumerate(test_dataloader):\n",
    "            \n",
    "            b_input_ids = batch[0].type(torch.LongTensor).to(device)\n",
    "            b_input_mask = batch[1].type(torch.LongTensor).to(device)\n",
    "            b_labels = batch[2].type(torch.LongTensor).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels\n",
    "                )\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            total_test_loss += output.loss.item()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            result = torch.argmax(output.logits, 1)\n",
    "            correct_tensor = result.eq(b_labels.data.view_as(result))\n",
    "            correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            if idx == 0:\n",
    "                total_ground_truth = b_labels.data\n",
    "                total_pred = result\n",
    "            else:\n",
    "                total_ground_truth = torch.cat((total_ground_truth, b_labels.data), 0)\n",
    "                total_pred = torch.cat((total_pred, result), 0)\n",
    "\n",
    "            # calculate test accuracy for each object class\n",
    "            for i in range(len(b_labels)): ## TODO\n",
    "                label = b_labels.data[i]\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        print('Test Loss: {:.6f}\\n'.format(avg_test_loss))\n",
    "        mlflow.log_metric(\"avg_test_loss\", avg_test_loss)\n",
    "\n",
    "        \n",
    "        for i in range(len(params['class_names'])):\n",
    "            if class_total[i] > 0:\n",
    "                class_accuracy = 100 * class_correct[i] / class_total[i]\n",
    "                print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                    params['class_names'][i], class_accuracy,\n",
    "                    np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "                mlflow.log_metric(f\"test_accuracy_{params['class_names'][i]}\", class_accuracy)\n",
    "            else:\n",
    "                print('Test Accuracy of %5s: N/A (no training examples)' % (params['class_names'][i]))\n",
    "\n",
    "        # test_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
    "        # print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        #     test_accuracy,\n",
    "        #     np.sum(class_correct), np.sum(class_total)))\n",
    "        # mlflow.log_metric(\"test_accuracy_overall\", test_accuracy)\n",
    "\n",
    "        accuracy_score_number = accuracy_score(total_ground_truth.cpu(), total_pred.cpu())\n",
    "        recall_score_number = recall_score(total_ground_truth.cpu(), total_pred.cpu())\n",
    "        precision_score_number = precision_score(total_ground_truth.cpu(), total_pred.cpu())\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy_score_number)\n",
    "        mlflow.log_metric(\"test_recall\", recall_score_number)\n",
    "        mlflow.log_metric(\"test_precision\", precision_score_number)\n",
    "\n",
    "        print('\\nTest Accuracy (Overall): %2d%%' % (100 * accuracy_score_number))\n",
    "        print('\\nTest Recall (Overall): %2d%%' % (100 * recall_score_number))\n",
    "        print('\\nTest precision (Overall): %2d%%' % (100 * precision_score_number))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        # training_stats.append(\n",
    "        #     {\n",
    "        #         'epoch': epoch_i + 1,\n",
    "        #         'Training Loss': avg_train_loss,\n",
    "        #         'Valid. Loss': avg_val_loss,\n",
    "        #         'Valid. Accur.': accuracy_score_number,\n",
    "        #         'Training Time': training_time,\n",
    "        #         'Validation Time': validation_time\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_flow(run_name) -> None:\n",
    "    # Set mlflow\n",
    "    TRACKING_SERVER_HOST = \"localhost\"\n",
    "    mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:5000\")\n",
    "    mlflow.set_experiment(\"Sentiment_Analisys\")\n",
    "\n",
    "    # Feature engineering step\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    prep_train_texts = [data_preprocessing(text) for text in train_texts]\n",
    "    max_sentence_length = get_max_sentence_length(prep_train_texts, tokenizer)\n",
    "    input_ids, attention_masks, labels = bert_preprocessing(prep_train_texts, tokenizer, max_sentence_length, train_labels)\n",
    "    \n",
    "    prep_test_texts = [data_preprocessing(text) for text in test_texts]\n",
    "    input_ids_test, attention_masks_test, labels_test = bert_preprocessing(prep_test_texts, tokenizer, max_sentence_length, test_labels)\n",
    "\n",
    "    train_dataloader, validation_dataloader, test_dataloader = get_data_loaders(input_ids, attention_masks, labels, input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "    # Select model to train\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                        # You can increase this for multi-class tasks.\n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        \n",
    "    )\n",
    "    model.cuda()\n",
    "    \n",
    "    # Train model\n",
    "    params = {\n",
    "            \"class_names\": ['negative', 'positive'],\n",
    "            \"n_epochs\": 2,\n",
    "            \"criterion\": \"CrossEntropyLoss\",\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"run_name\": run_name\n",
    "        }\n",
    "    train_test_model(train_dataloader, validation_dataloader, test_dataloader, model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisg\\Documents\\projects\\nlp_amazon_reviews\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,000 training samples\n",
      "2,000 validation samples\n",
      "2,000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\luisg\\Documents\\projects\\nlp_amazon_reviews\\.venv\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Training epoch took: 0:03:08\n",
      "\n",
      "Running Validation...\n",
      "Epoch: 0 \tTraining Loss: 0.353920 \tValidation Loss: 0.254892 \t\\Time: 0:00:21\n",
      "Validation loss decreased (inf --> 0.254892).  Saving model ...\n",
      "\n",
      "Running Testing...\n",
      "Test Loss: 0.264410\n",
      "\n",
      "Test Accuracy of negative: 88% (845/954)\n",
      "Test Accuracy of positive: 89% (940/1046)\n",
      "\n",
      "Test Accuracy (Overall): 89%\n",
      "\n",
      "Test Recall (Overall): 89%\n",
      "\n",
      "Test precision (Overall): 89%\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "main_flow(run_name='Prueba2_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ../models/bert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/bert\\\\tokenizer_config.json',\n",
       " '../models/bert\\\\special_tokens_map.json',\n",
       " '../models/bert\\\\vocab.txt',\n",
       " '../models/bert\\\\added_tokens.json',\n",
       " '../models/bert\\\\tokenizer.json')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = '../models/bert'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    '../models/bert', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    \n",
    ")\n",
    "\n",
    "new_tokenizer = AutoTokenizer.from_pretrained('../models/bert', do_lower_case=True)\n",
    "\n",
    "new_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'It was good but couldve been better. Great'\n",
    "test_text = 'It is the worst product I have ever bought'\n",
    "test_text = data_preprocessing(test_text)\n",
    "tokens = new_tokenizer.encode(test_text, return_tensors='pt')\n",
    "result = new_model(tokens.cpu())\n",
    "result = int(torch.argmax(result.logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9501, -2.6430]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = 'It was good but couldve been better. Great'\n",
    "test_text = 'It is the worst product I have ever bought'\n",
    "test_text = data_preprocessing(test_text)\n",
    "tokens = new_tokenizer.encode(test_text, return_tensors='pt')\n",
    "result = new_model(tokens.cpu())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(result.logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9501, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
